# End to End Data Science project



### Workflows--ML Pipeline

1. Data Ingestion
2. Data Validation
3. Data Transformation-- Feature Engineering,Data Preprocessing
4. Model Trainer
5. Model Evaluation- MLFLOW,Dagshub



## Workflows

1. Update config.yaml
2. Update schema.yaml
3. Update params.yaml
4. Update the entity
5. Update the configuration manager in src config   //âš™ï¸Python helpers to load/parse config.yaml and schema.yaml.
6. Update the components
7. Update the pipeline 
8. Update the main.py




ğŸ“‚ Project Structure & Roles

Top-level files/folders

ğŸ“ config/
âœ¨ config.yaml â†’ Central configuration file for paths, model hyperparameters, database connections, etc.
âœ¨ Keeps your pipeline flexible without hardcoding values.

ğŸ“ experiment/
âœ¨ Stores experiment runs (metrics, artifacts, models, etc.).
âœ¨ Helps in experiment tracking if youâ€™re not using MLflow/W&B.

ğŸ“ Logs/
âœ¨ Contains runtime logs (training, evaluation, error logs).
âœ¨ Useful for debugging and monitoring pipeline runs.

ğŸ“ src/DataScience/ (main source code package)
âœ¨ Core ML codebase. Letâ€™s break it down:

   - ğŸ“ components/ â†’ ğŸ§© Reusable building blocks (e.g., data ingestion, preprocessing, training, evaluation). Each  script handles one major step.

   - ğŸ“ config/ â†’ âš™ï¸ Python helpers to load/parse config.yaml and schema.yaml.

   - ğŸ“ constants/ â†’ ğŸ“ Store global constants (like file paths, feature names, column mappings).

   - ğŸ“ entity/ â†’ ğŸ—‚ï¸ ONLY DEFINE data entities (e.g., input/output dataclasses, structured schemas).

   - ğŸ“ pipeline/ â†’ ğŸ”— Orchestration scripts that connect components (end-to-end ML pipeline).

   - ğŸ“ utils/ â†’ ğŸ› ï¸ Helper functions (logging, file I/O, validation, metrics calculation).

ğŸ“„ __init__.py â†’ ğŸ“¦ Makes DataScience a Python package.

ğŸ“ templates/
âœ¨ ğŸ“‘ Jinja2 or YAML/JSON templates for generating config files, reports, or metadata.

ğŸ“„ .gitignore
âœ¨ ğŸš« Excludes unnecessary files (logs, models, venv, cache) from git commits.

ğŸ“„ Dockerfile
âœ¨ ğŸ³ Defines containerization setup. Ensures reproducibility across environments (same dependencies, system libs).

ğŸ“„ main.py
âœ¨ â–¶ï¸ Entry point for running the entire ML pipeline.
âœ¨ Example: python main.py train could trigger data ingestion â†’ preprocessing â†’ training â†’ evaluation.

ğŸ“„ params.yaml
âœ¨ ğŸšï¸ Stores model-specific hyperparameters (learning rate, epochs, batch size).
âœ¨ Often used with pipeline orchestrators like DVC.

ğŸ“„ Readme.md
âœ¨ ğŸ“– Project documentation, setup instructions, workflow description.

ğŸ“„ requirement.txt
âœ¨ ğŸ“¦ Python dependencies list for pip installation.

ğŸ“„ schema.yaml
âœ¨ ğŸ“ Defines schema/validation rules for datasets (column names, types, ranges).
âœ¨ Helps catch data drift and schema mismatches.